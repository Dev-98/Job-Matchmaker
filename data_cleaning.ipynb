{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b92fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfminer.six\n",
    "import re\n",
    "import pandas as pd\n",
    "# from pdfminer.high_level import extract_text\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import torch, pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0689f4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')  # Downloads the Punkt tokenizer models\n",
    "nltk.download('stopwords')  # Downloads the stopwords corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a206a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"newdata.csv\")\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "embed = []\n",
    "for jd in data[\"Description\"]:\n",
    "    embed.append(model.encode(jd, convert_to_tensor=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "264fb72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NextGen Techno Ventures Private Limited\n",
      "Across The Globe (ATG)\n",
      "Jarvics Technologies\n",
      "Impact Analytics\n",
      "Mark Web Solutions\n"
     ]
    }
   ],
   "source": [
    "# embed[0:3]\n",
    "data = pd.read_csv(\"newdata.csv\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(data['Company_Name'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "940649fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"description_embedding\"] = embed\n",
    "\n",
    "data.to_csv(\"newdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4245ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "def get_similarity_text(jd, resume_embedding):\n",
    "    '''finding similarity between two texts'''\n",
    "    # calculating cosine similarity\n",
    "    jd_embedding = model.encode(jd, convert_to_tensor=True)\n",
    "\n",
    "    cos_scores = util.cos_sim(jd_embedding, resume_embedding)[0]\n",
    "    top_results = torch.topk(cos_scores, k=1)\n",
    "    score = top_results[0].numpy()[0]\n",
    "\n",
    "    return score\n",
    "\n",
    "resume = \"python javascript agile leadership scrum master computer vision generative ai git github mlops kubernetes mongodb firebase db pinecone mysql machine learning deep learning google cloud platform microsoft azure vector db docker flask fastapi professional experience dataknobs ml engineer june present automated key processes kreatewebsite major product enhance efficiency uplifted machine learning project predictive model precision made usable converting old written functions new one collaborate senior developers update website create new features open source contributor mlflow contributor august august contributed key functionality got merged administrator mlflow google cloud google cloud facilitator may july acquired proficiency docker mlops kubernetes kubernetes relevant projects sign language tutor march present used learning sign language fun interactive way chakla controller asphalt january january innovative racing game controlled unique physical interface round flat board blue square uses opencv computer vision techniques translate board movements game actions medsarthi january january helping seniors understand medications simple image upload voice enabled explanations education maharishi dayanand university rohtak bachelor computer science artificial intelligence rajokari institute technology dseu diploma information technology enabled service management\"\n",
    "\n",
    "# newdata = pandas.DataFrame()\n",
    "\n",
    "\n",
    "load_data = pandas.read_csv(\"newdata.csv\")\n",
    "resume_embedding = model.encode(resume, convert_to_tensor=True)   \n",
    "# print(get_similarity_text(jd,text))\n",
    "# resume_content = nlp(fetch_skills(\"Aman resume for y2.pdf\"))\n",
    "l = []\n",
    "for desc in load_data[\"Description\"]:\n",
    "    # desc_embed = np.array(desc.replace('\"',''))\n",
    "    # desc_embed = torch.tensor(desc_embed)\n",
    "    # print(desc_embed)\n",
    "    l.append(get_similarity_text(desc,resume_embedding))\n",
    "\n",
    "final = sorted(list(enumerate(l)), reverse=True, key=lambda x: x[1])[0:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc4cbcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data['Description'][4]\n",
    "from main import chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adb4009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = \"python javascript agile leadership scrum master computer vision generative ai git github mlops kubernetes mongodb firebase db pinecone mysql machine learning deep learning google cloud platform microsoft azure vector db docker flask fastapi professional experience dataknobs ml engineer  administrator mlflow google cloud google cloud facilitator may july acquired proficiency docker mlops kubernetes innovative racing game controlled unique physical interface round flat board blue square uses opencv computer vision\"\n",
    "\n",
    "jd = \"Selected intern's day-to-day responsibilities include:\\n\\n1. Work on Java & Core Java\\n2. Work on object-oriented programming (OOP) concepts and patterns\\n3. Work on Java-based microservice architecture\\n4. Work on the project using Spring Java frameworks\\n5. Handle the service-oriented architecture/web services\\n\\nRequirements:\\n\\n1. Have some understanding of docker, and Gradle build tools\\n2. Understanding of the git version control system. AngularJS , Hibernate (Java) , Java , MySQL , Spring MVC\"\n",
    "\n",
    "ans = chat_completion(resume,jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d0f0f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"JDMatch\":\"10%\",\"MissingKeywords\":[\"Java\",\"Core Java\",\"object-oriented programming\",\"OOP\",\"Java-based microservice architecture\",\"Spring Java frameworks\",\"service-oriented architecture\",\"web services\",\"docker\",\"Gradle build tools\",\"git version control system\",\"AngularJS\",\"Hibernate\",\"MySQL\",\"Spring MVC\"],\"Profile Summary\":\"The resume does not match the job description as it lacks experience in Java, Core Java, object-oriented programming, Spring Java frameworks, service-oriented architecture, web services, docker, Gradle build tools, git version control system, AngularJS, Hibernate, MySQL, and Spring MVC.\"}'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ans = dict(ans)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868b371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import base64, re, os\n",
    "\n",
    "def pdf_reader(pdf_file):\n",
    "\n",
    "    text = extract_text(pdf_file).lower()\n",
    "    # skill = text.split(\"skills\")[1]\n",
    "    # os.remove(pdf_file)\n",
    "#     get keywords\n",
    "    keywords = \" \".join(re.findall(r'[a-zA-Z]\\w+',text.lower()))\n",
    "\n",
    "    token_text = word_tokenize(keywords)\n",
    "    stop_words = stopwords.words('english')\n",
    "    clean_text = []\n",
    "    for i in token_text:\n",
    "        if i not in stop_words:\n",
    "            clean_text.append(i)\n",
    "    clean_text = \" \".join(clean_text)\n",
    "    \n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "    clean_text = re.sub(pattern, '', clean_text).replace(\"\\n\", \"\")\n",
    "    \n",
    "        # Define a regular expression pattern to match numbers\n",
    "    pattern2 = r'\\d+'\n",
    "\n",
    "    # Remove numbers from the text using regex substitution\n",
    "    text_without_numbers = re.sub(pattern2, '', clean_text)\n",
    "    \n",
    "    return text_without_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ff71242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev machine learning engineer faridabad haryana devparker gmail com linkedin github twitter kaggle core skills python javascript agile leadership scrum master computer vision generative ai git github mlops kubernetes mongodb firebase db pinecone mysql machine learning deep learning google cloud platform microsoft azure vector db docker flask fastapi professional experience dataknobs ml engineer june present automated key processes kreatewebsite major product enhance efficiency uplifted machine learning project predictive model precision made usable converting old written functions new one collaborate senior developers update website create new features open source contributor mlflow contributor august august contributed key functionality got merged administrator mlflow google cloud google cloud facilitator may july acquired proficiency docker mlops kubernetes kubernetes relevant projects sign language tutor march present used learning sign language fun interactive way chakla controller asphalt january january innovative racing game controlled unique physical interface round flat board blue square uses opencv computer vision techniques translate board movements game actions medsarthi january january helping seniors understand medications simple image upload voice enabled explanations education maharishi dayanand university rohtak bachelor computer science artificial intelligence rajokari institute technology dseu diploma information technology enabled service management'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader(\"dev.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b3fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "load_data = pd.read_csv(\"data.csv\")\n",
    "load_data.reset_index(inplace=True)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "content = nlp(fetch_skills(\"Aman resume for y2.pdf\"))\n",
    "l = []\n",
    "for index, i in enumerate(load_data[\"Description\"]):\n",
    "    l.append(content.similarity(nlp(i)))\n",
    "final = sorted(list(enumerate(l)), reverse=True, key=lambda x: x[1])[0:6]\n",
    "for i, j in final:\n",
    "    print(load_data.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ccc1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data[\"content\"] = load_data[\"Description\"] + load_data[\"JobTitles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4023ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=5000,stop_words=\"english\")\n",
    "vectors = cv.fit_transform(load_data[\"content\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4754a8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c036fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6fa8de4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.17173031, 0.        , ..., 0.2362571 , 0.13328683,\n",
       "        0.12879861],\n",
       "       [0.17173031, 1.        , 0.        , ..., 0.16586995, 0.2211825 ,\n",
       "        0.3100325 ],\n",
       "       [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.2362571 , 0.16586995, 0.        , ..., 1.        , 0.21222324,\n",
       "        0.20141487],\n",
       "       [0.13328683, 0.2211825 , 0.        , ..., 0.21222324, 1.        ,\n",
       "        0.37188078],\n",
       "       [0.12879861, 0.3100325 , 0.        , ..., 0.20141487, 0.37188078,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = cosine_similarity(vectors)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01aa07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = sorted(list(enumerate(similarity[0])), reverse=True, key=lambda x: x[1]) [1:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ac5852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv() ## load all our environment variables\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GENAI_API_KEY\"))\n",
    "\n",
    "# Example usage\n",
    "def get_gemini_repsonse(r_text:str,jd:str) :\n",
    "    input = f\"\"\"Act like a skilled or very experience ATS(Application Tracking System)\n",
    "          with a deep understanding of various software fields. Your task is to evaluate the resume based on the given job description.\n",
    "          And provide an analysis of the resume based on the given job description and\n",
    "          best assistance for improving the resumes. Point out the missing keywords in resume with high accuracy\n",
    "          resume:{r_text}\n",
    "          description:{jd}\n",
    "\n",
    "          I want the response in dictionary format having the structure\n",
    "          (MissingKeywords:[], Analysis :'')\n",
    "        \"\"\"\n",
    "    model=genai.GenerativeModel('gemini-pro')\n",
    "    response=model.generate_content(input)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6db9ad3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\n{\\n    \"MissingKeywords\": [\"docker\", \"gradle\", \"angularjs\", \"hibernate\"],\\n    \"Analysis\": \"The resume lacks specific keywords mentioned in the job description. The candidate has experience in Python, JavaScript, computer vision, generative AI, Git, GitHub, MLOps, Kubernetes, MongoDB, Firebase DB, Pinecone, MySQL, machine learning, and deep learning. However, the resume does not highlight any experience in Java & Core Java, OOP concepts and patterns, Java-based microservice architecture, Spring Java frameworks, or service-oriented architecture/web services. To improve the resume, the candidate should include relevant projects or experiences that demonstrate their proficiency in these areas.\"\\n}\\n```'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_text = \"\"\"\n",
    "skills python javascript computer vision generative ai git github mlops kubernetes mongodb firebase db pinecone mysql machine learning deep learning google cloud platform microsoft azure vector db docker flask fastapi professional experience dataknobs ml engineer source contributor mlflow contributor august august contributed key functionality got merged administrator mlflow google cloud google cloud facilitator may july acquired proficiency docker mlops kubernetes kubernetes relevant projects sign language tutor march present used learning sign language fun interactive way chakla controller asphalt january january innovative racing game controlled unique physical interface round flat board blue square uses opencv computer vision techniques translate board movements game actions medsarthi january january helping seniors understand medications simple image upload voice enabled explanations education maharishi dayanand university rohtak bachelor computer science artificial intelligence rajokari institute technology dseu diploma information technology enabled service management\n",
    "\"\"\"\n",
    "jd = \"Selected intern's day-to-day responsibilities include:\\n\\n1. Work on Java & Core Java\\n2. Work on object-oriented programming (OOP) concepts and patterns\\n3. Work on Java-based microservice architecture\\n4. Work on the project using Spring Java frameworks\\n5. Handle the service-oriented architecture/web services\\n\\nRequirements:\\n\\n1. Have some understanding of docker, and Gradle build tools\\n2. Understanding of the git version control system. AngularJS , Hibernate (Java) , Java , MySQL , Spring MVC\"\n",
    "\n",
    "\n",
    "temp = f\"\"\"Act Like a skilled or very experience ATS(Application Tracking System)\n",
    "          with a deep understanding of Data science, web development. Your task is to evaluate the resume based on the given job description.\n",
    "          You must consider the job market is very competitive and you should provide \n",
    "          best assistance for improving thr resumes. Assign the percentage Matching based \n",
    "          on Jd and\n",
    "          the missing keywords with high accuracy\n",
    "          resume:{r_text}\n",
    "          description:{jd}\n",
    "\n",
    "          I want the response having the structure\n",
    "          (\"JDMatch\":int,\"MissingKeywords\":[],\"Profile Summary\":\"\")\n",
    "        \"\"\"\n",
    "scores = get_gemini_repsonse(r_text,jd)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27e07655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"The provided resume does not include any information about the candidate\\'s experience or skills in Java, Core Java, OOP, Spring Java, Gradle, AngularJS, Hibernate, MySQL, or Spring MVC. These are all essential requirements for the position, so the candidate would need to add this information to their resume in order to be considered.\"\\n}\\n```'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = scores.split(':')[2]\n",
    "from pinecone import Pinecone\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d31dd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import re, os, requests\n",
    "from pinecone import Pinecone\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "embedding_url = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# genai.configure(api_key=os.getenv(\"GENAI_API_KEY\"))\n",
    "\n",
    "def generate_embedding(text: str) -> list[float]:\n",
    "\n",
    "\tresponse = requests.post(\n",
    "\t\tembedding_url,\n",
    "\t\theaders={\"Authorization\": f\"Bearer {hf_token}\"},\n",
    "\t\tjson={\"inputs\": text})\n",
    "\n",
    "\tif response.status_code != 200:\n",
    "\t\traise ValueError(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "\treturn response.json()\n",
    "\n",
    "def get_jobs_new(input_query:str,namespace:str,k=5):\n",
    "    \n",
    "    pine = Pinecone(api_key=os.getenv('PINECONE_KEY'))\n",
    "    index = pine.Index(os.getenv('PINECONE_INDEX'))\n",
    "\n",
    "    # input_query = pdf_reader(input_query)\n",
    "    input_embed = generate_embedding(input_query)\n",
    "\n",
    "    pinecone_resp = index.query(vector=input_embed, top_k=k, include_metadata=True, namespace=namespace)\n",
    "    if not pinecone_resp['matches']:\n",
    "        # print(pinecone_resp)\n",
    "        return \"No Jobs Found, Maybe you should learn more skills and do more projects to get more jobs\"\n",
    "\n",
    "    context = []\n",
    "    scores = []\n",
    "    for i in range(len(pinecone_resp['matches'])):\n",
    "\n",
    "        scores.append(pinecone_resp['matches'][i][\"score\"] )\n",
    "        context.append(pinecone_resp['matches'][i]['metadata'])\n",
    "    \n",
    "    return scores,context\n",
    "\n",
    "# def job_searcher(text:str,domain:str) :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b352c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc, jds = get_jobs_new(r_text,\"internship\",4)\n",
    "\n",
    "print(\"Scores : \", sc)\n",
    "print(\"\\n Jobs : \", jds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
