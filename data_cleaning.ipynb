{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b92fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfminer.six\n",
    "import re\n",
    "import pandas as pd\n",
    "# from pdfminer.high_level import extract_text\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import torch, pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0689f4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')  # Downloads the Punkt tokenizer models\n",
    "nltk.download('stopwords')  # Downloads the stopwords corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a206a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"newdata.csv\")\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "embed = []\n",
    "for jd in data[\"Description\"]:\n",
    "    embed.append(model.encode(jd, convert_to_tensor=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "264fb72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NextGen Techno Ventures Private Limited\n",
      "Across The Globe (ATG)\n",
      "Jarvics Technologies\n",
      "Impact Analytics\n",
      "Mark Web Solutions\n"
     ]
    }
   ],
   "source": [
    "# embed[0:3]\n",
    "data = pd.read_csv(\"newdata.csv\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(data['Company_Name'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "940649fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"description_embedding\"] = embed\n",
    "\n",
    "data.to_csv(\"newdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4245ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "def get_similarity_text(jd, resume_embedding):\n",
    "    '''finding similarity between two texts'''\n",
    "    # calculating cosine similarity\n",
    "    jd_embedding = model.encode(jd, convert_to_tensor=True)\n",
    "\n",
    "    cos_scores = util.cos_sim(jd_embedding, resume_embedding)[0]\n",
    "    top_results = torch.topk(cos_scores, k=1)\n",
    "    score = top_results[0].numpy()[0]\n",
    "\n",
    "    return score\n",
    "\n",
    "resume = \"python javascript agile leadership scrum master computer vision generative ai git github mlops kubernetes mongodb firebase db pinecone mysql machine learning deep learning google cloud platform microsoft azure vector db docker flask fastapi professional experience dataknobs ml engineer june present automated key processes kreatewebsite major product enhance efficiency uplifted machine learning project predictive model precision made usable converting old written functions new one collaborate senior developers update website create new features open source contributor mlflow contributor august august contributed key functionality got merged administrator mlflow google cloud google cloud facilitator may july acquired proficiency docker mlops kubernetes kubernetes relevant projects sign language tutor march present used learning sign language fun interactive way chakla controller asphalt january january innovative racing game controlled unique physical interface round flat board blue square uses opencv computer vision techniques translate board movements game actions medsarthi january january helping seniors understand medications simple image upload voice enabled explanations education maharishi dayanand university rohtak bachelor computer science artificial intelligence rajokari institute technology dseu diploma information technology enabled service management\"\n",
    "\n",
    "# newdata = pandas.DataFrame()\n",
    "\n",
    "\n",
    "load_data = pandas.read_csv(\"newdata.csv\")\n",
    "resume_embedding = model.encode(resume, convert_to_tensor=True)   \n",
    "# print(get_similarity_text(jd,text))\n",
    "# resume_content = nlp(fetch_skills(\"Aman resume for y2.pdf\"))\n",
    "l = []\n",
    "for desc in load_data[\"Description\"]:\n",
    "    # desc_embed = np.array(desc.replace('\"',''))\n",
    "    # desc_embed = torch.tensor(desc_embed)\n",
    "    # print(desc_embed)\n",
    "    l.append(get_similarity_text(desc,resume_embedding))\n",
    "\n",
    "final = sorted(list(enumerate(l)), reverse=True, key=lambda x: x[1])[0:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc4cbcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data['Description'][4]\n",
    "from main import chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adb4009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = \"python javascript agile leadership scrum master computer vision generative ai git github mlops kubernetes mongodb firebase db pinecone mysql machine learning deep learning google cloud platform microsoft azure vector db docker flask fastapi professional experience dataknobs ml engineer  administrator mlflow google cloud google cloud facilitator may july acquired proficiency docker mlops kubernetes innovative racing game controlled unique physical interface round flat board blue square uses opencv computer vision\"\n",
    "\n",
    "jd = \"Selected intern's day-to-day responsibilities include:\\n\\n1. Work on Java & Core Java\\n2. Work on object-oriented programming (OOP) concepts and patterns\\n3. Work on Java-based microservice architecture\\n4. Work on the project using Spring Java frameworks\\n5. Handle the service-oriented architecture/web services\\n\\nRequirements:\\n\\n1. Have some understanding of docker, and Gradle build tools\\n2. Understanding of the git version control system. AngularJS , Hibernate (Java) , Java , MySQL , Spring MVC\"\n",
    "\n",
    "ans = chat_completion(resume,jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d0f0f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"JDMatch\":\"10%\",\"MissingKeywords\":[\"Java\",\"Core Java\",\"object-oriented programming\",\"OOP\",\"Java-based microservice architecture\",\"Spring Java frameworks\",\"service-oriented architecture\",\"web services\",\"docker\",\"Gradle build tools\",\"git version control system\",\"AngularJS\",\"Hibernate\",\"MySQL\",\"Spring MVC\"],\"Profile Summary\":\"The resume does not match the job description as it lacks experience in Java, Core Java, object-oriented programming, Spring Java frameworks, service-oriented architecture, web services, docker, Gradle build tools, git version control system, AngularJS, Hibernate, MySQL, and Spring MVC.\"}'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ans = dict(ans)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "868b371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import base64, re, os\n",
    "\n",
    "def pdf_reader(pdf_file):\n",
    "\n",
    "    text = extract_text(pdf_file).lower()\n",
    "    awards = text.split(\"awards\")[0]\n",
    "    # try:\n",
    "        # skills = text.split(\"skills\")[1]\n",
    "    keywords = \" \".join(re.findall(r'[a-zA-Z]\\w+',awards))\n",
    "    #     print('awards mila')\n",
    "    # except Exception as e:\n",
    "    #     keywords = \" \".join(re.findall(r'[a-zA-Z]\\w+',text))\n",
    "    # os.remove(pdf_file)\n",
    "#     get keywords\n",
    "    # keywords = \" \".join(re.findall(r'[a-zA-Z]\\w+',text))\n",
    "\n",
    "    token_text = word_tokenize(keywords)\n",
    "    stop_words = stopwords.words('english')\n",
    "    clean_text = []\n",
    "    for i in token_text:\n",
    "        if i not in stop_words:\n",
    "            clean_text.append(i)\n",
    "    clean_text = \" \".join(clean_text)\n",
    "    \n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "    clean_text = re.sub(pattern, '', clean_text).replace(\"\\n\", \"\")\n",
    "    \n",
    "        # Define a regular expression pattern to match numbers\n",
    "    pattern2 = r'\\d+'\n",
    "\n",
    "    # Remove numbers from the text using regex substitution\n",
    "    text_without_numbers = re.sub(pattern2, '', clean_text)\n",
    "    \n",
    "    return text_without_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ff71242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mayank divakar machine learning engineer full stack web developer contact mayankdivakar gmail com new delhi education meri college engineering machine learning engineer data knobs excel python deep learning computer vision specializing developing comprehensive machine learning solutions deploying models practical applications experience includes leading ai projects college diverse clients created impactful solutions various challenges skill set extends building web applications interactive platforms emphasizing personalization user empowerment committed continual learning harness data driven insights drive meaningful results ai web development making valuable impact tech industry tech cse army public school class xii skills python numpy machine learning computer vision django react js mysql mongodb kubernetes opencv google cloud console communication skills html css generative al deep learning app engine render api three js nodejs bigquery gcp problem solving time management streamlit flask english work experience machine learning engineer dataknobs new delhi august current engineered api pdf question answering strong emphasis user data privacy system extracts text questions pdf documents provides precise answers using machine learning models ensures anonymization personal details also securely stores processed data google cloud storage buckets assigning user dedicated secure storage space designed implemented api applies anonymity closure diversity privacy protection techniques csv datasets system takes user input form csv data performs anonymization operations returns anonymized dataset ensuring sensitive information protected implemented automated data pipelines collecting preprocessing large datasets multiple sources projects data margdarshan sih project developed machine learning recommendation engine analyzes users interests skills goals suggest suitable career paths educational programs job opportunities enabled students create personalized profiles input academic extracurricular achievements receive tailored advice potential career choices integrated data analytics track user engagement career choices effectiveness recommendations allowing continuous improvement pdf based questioning deployed https free kreatebots com created web based chatbot takes pdf file knowledge base answer question related pdf tf idf vectors cosign similarity specify length starting end index answer well question answer prompts anonymized help ner saved google cloud buckets linkedin com mayankprogrammer med sathi ocr github com mayankiscoding created application using python kiwi captures images webcam help opencv help easyocr text image retrieved cleaned help basic nlp cleaned text compared database provided help gtts audio output generated'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader(\"mayank_resume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b3fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "load_data = pd.read_csv(\"data.csv\")\n",
    "load_data.reset_index(inplace=True)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "content = nlp(fetch_skills(\"Aman resume for y2.pdf\"))\n",
    "l = []\n",
    "for index, i in enumerate(load_data[\"Description\"]):\n",
    "    l.append(content.similarity(nlp(i)))\n",
    "final = sorted(list(enumerate(l)), reverse=True, key=lambda x: x[1])[0:6]\n",
    "for i, j in final:\n",
    "    print(load_data.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ccc1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data[\"content\"] = load_data[\"Description\"] + load_data[\"JobTitles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4023ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=5000,stop_words=\"english\")\n",
    "vectors = cv.fit_transform(load_data[\"content\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4754a8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c036fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6fa8de4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.17173031, 0.        , ..., 0.2362571 , 0.13328683,\n",
       "        0.12879861],\n",
       "       [0.17173031, 1.        , 0.        , ..., 0.16586995, 0.2211825 ,\n",
       "        0.3100325 ],\n",
       "       [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.2362571 , 0.16586995, 0.        , ..., 1.        , 0.21222324,\n",
       "        0.20141487],\n",
       "       [0.13328683, 0.2211825 , 0.        , ..., 0.21222324, 1.        ,\n",
       "        0.37188078],\n",
       "       [0.12879861, 0.3100325 , 0.        , ..., 0.20141487, 0.37188078,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = cosine_similarity(vectors)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01aa07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = sorted(list(enumerate(similarity[0])), reverse=True, key=lambda x: x[1]) [1:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ac5852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv() ## load all our environment variables\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GENAI_API_KEY\"))\n",
    "\n",
    "# Example usage\n",
    "def get_gemini_repsonse(r_text:str,jd:str) :\n",
    "    input = f\"\"\"Act like a skilled or very experience ATS(Application Tracking System)\n",
    "          with a deep understanding of various software fields. Your task is to evaluate the resume based on the given job description.\n",
    "          And provide an analysis of the resume based on the given job description and\n",
    "          best assistance for improving the resumes. Point out the missing keywords in resume with high accuracy\n",
    "          resume:{r_text}\n",
    "          description:{jd}\n",
    "\n",
    "          I want the response in dictionary format having the structure\n",
    "          (MissingKeywords:[], Analysis :'')\n",
    "        \"\"\"\n",
    "    model=genai.GenerativeModel('gemini-pro')\n",
    "    response=model.generate_content(input)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6db9ad3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\n{\\n    \"MissingKeywords\": [\"docker\", \"gradle\", \"angularjs\", \"hibernate\"],\\n    \"Analysis\": \"The resume lacks specific keywords mentioned in the job description. The candidate has experience in Python, JavaScript, computer vision, generative AI, Git, GitHub, MLOps, Kubernetes, MongoDB, Firebase DB, Pinecone, MySQL, machine learning, and deep learning. However, the resume does not highlight any experience in Java & Core Java, OOP concepts and patterns, Java-based microservice architecture, Spring Java frameworks, or service-oriented architecture/web services. To improve the resume, the candidate should include relevant projects or experiences that demonstrate their proficiency in these areas.\"\\n}\\n```'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_text = \"\"\"\n",
    "skills python javascript computer vision generative ai git github mlops kubernetes mongodb firebase db pinecone mysql machine learning deep learning google cloud platform microsoft azure vector db docker flask fastapi professional experience dataknobs ml engineer source contributor mlflow contributor august august contributed key functionality got merged administrator mlflow google cloud google cloud facilitator may july acquired proficiency docker mlops kubernetes kubernetes relevant projects sign language tutor march present used learning sign language fun interactive way chakla controller asphalt january january innovative racing game controlled unique physical interface round flat board blue square uses opencv computer vision techniques translate board movements game actions medsarthi january january helping seniors understand medications simple image upload voice enabled explanations education maharishi dayanand university rohtak bachelor computer science artificial intelligence rajokari institute technology dseu diploma information technology enabled service management\n",
    "\"\"\"\n",
    "jd = \"Selected intern's day-to-day responsibilities include:\\n\\n1. Work on Java & Core Java\\n2. Work on object-oriented programming (OOP) concepts and patterns\\n3. Work on Java-based microservice architecture\\n4. Work on the project using Spring Java frameworks\\n5. Handle the service-oriented architecture/web services\\n\\nRequirements:\\n\\n1. Have some understanding of docker, and Gradle build tools\\n2. Understanding of the git version control system. AngularJS , Hibernate (Java) , Java , MySQL , Spring MVC\"\n",
    "\n",
    "\n",
    "temp = f\"\"\"Act Like a skilled or very experience ATS(Application Tracking System)\n",
    "          with a deep understanding of Data science, web development. Your task is to evaluate the resume based on the given job description.\n",
    "          You must consider the job market is very competitive and you should provide \n",
    "          best assistance for improving thr resumes. Assign the percentage Matching based \n",
    "          on Jd and\n",
    "          the missing keywords with high accuracy\n",
    "          resume:{r_text}\n",
    "          description:{jd}\n",
    "\n",
    "          I want the response having the structure\n",
    "          (\"JDMatch\":int,\"MissingKeywords\":[],\"Profile Summary\":\"\")\n",
    "        \"\"\"\n",
    "scores = get_gemini_repsonse(r_text,jd)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27e07655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"The provided resume does not include any information about the candidate\\'s experience or skills in Java, Core Java, OOP, Spring Java, Gradle, AngularJS, Hibernate, MySQL, or Spring MVC. These are all essential requirements for the position, so the candidate would need to add this information to their resume in order to be considered.\"\\n}\\n```'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = scores.split(':')[2]\n",
    "from pinecone import Pinecone\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d31dd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import re, os, requests\n",
    "from pinecone import Pinecone\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "embedding_url = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# genai.configure(api_key=os.getenv(\"GENAI_API_KEY\"))\n",
    "\n",
    "def generate_embedding(text: str) -> list[float]:\n",
    "\n",
    "\tresponse = requests.post(\n",
    "\t\tembedding_url,\n",
    "\t\theaders={\"Authorization\": f\"Bearer {hf_token}\"},\n",
    "\t\tjson={\"inputs\": text})\n",
    "\n",
    "\tif response.status_code != 200:\n",
    "\t\traise ValueError(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "\treturn response.json()\n",
    "\n",
    "def get_jobs_new(input_query:str,namespace:str,k=5):\n",
    "    \n",
    "    pine = Pinecone(api_key=os.getenv('PINECONE_KEY'))\n",
    "    index = pine.Index(os.getenv('PINECONE_INDEX'))\n",
    "\n",
    "    # input_query = pdf_reader(input_query)\n",
    "    input_embed = generate_embedding(input_query)\n",
    "\n",
    "    pinecone_resp = index.query(vector=input_embed, top_k=k, include_metadata=True, namespace=namespace)\n",
    "    if not pinecone_resp['matches']:\n",
    "        # print(pinecone_resp)\n",
    "        return \"No Jobs Found, Maybe you should learn more skills and do more projects to get more jobs\"\n",
    "\n",
    "    context = []\n",
    "    scores = []\n",
    "    for i in range(len(pinecone_resp['matches'])):\n",
    "\n",
    "        scores.append(pinecone_resp['matches'][i][\"score\"] )\n",
    "        context.append(pinecone_resp['matches'][i]['metadata'])\n",
    "    \n",
    "    return scores,context\n",
    "\n",
    "# def job_searcher(text:str,domain:str) :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b352c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores :  [0.555202723, 0.55518049, 0.543467879, 0.528032959]\n",
      "\n",
      " Jobs :  [{'JobTitles': 'General Artificial Intelligence/Machine Learning', 'company': 'Rugas Technologies Private Limited', 'link': 'https://internshala.com/internship/detail/general-artificial-intelligence-machine-learning-internship-in-bangalore-at-rugas-technologies-private-limited1709550813', 'skills': 'Algorithms , Amazon Web Services (AWS) , Computer Vision , Machine Learning , Natural Language Processing (NLP) , Neural Networks , Python', 'stipend': '₹ 15,000-20,000 /month'}, {'JobTitles': 'Machine Learning', 'company': 'Factacy.Ai', 'link': 'https://internshala.com/internship/detail/machine-learning-internship-in-gurgaon-at-factacyai1709288811', 'skills': 'Artificial Intelligence , Computer Vision , Data Science , Deep Learning , Machine Learning , Python', 'stipend': '₹ 12,000-15,000 /month'}, {'JobTitles': 'Machine Learning', 'company': 'Shubh International', 'link': 'https://internshala.com/internship/detail/work-from-home-part-time-machine-learning-internship-at-shubh-international1709128360', 'skills': '.NET , Data Analytics , Data Science , Deep Learning , Machine Learning , Natural Language Processing (NLP) , Python , R Programming', 'stipend': '₹ 14,000-16,000 /month'}, {'JobTitles': 'Machine Learning', 'company': 'Avaari', 'link': 'https://internshala.com/internship/detail/work-from-home-machine-learning-internship-at-avaari1709128081', 'skills': 'Data Analytics , Data Science , Deep Learning , Machine Learning , Natural Language Processing (NLP) , Python , R Programming', 'stipend': '₹ 15,000 /month'}]\n"
     ]
    }
   ],
   "source": [
    "sc, jds = get_jobs_new(r_text,\"internship\",4)\n",
    "\n",
    "print(\"Scores : \", sc)\n",
    "print(\"\\n Jobs : \", jds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6faf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jds[0]['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3674ab01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pine = Pinecone(api_key=os.getenv('PINECONE_KEY'))\n",
    "index = pine.Index(os.getenv('PINECONE_INDEX'))\n",
    "\n",
    "index.delete(delete_all=True,namespace=\"internship\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f9807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
